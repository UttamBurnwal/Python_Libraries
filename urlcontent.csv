,URL,Title,Content
0,https://practicaldatascience.co.uk/data-engineering/how-to-import-data-into-google-data-studio-using-python,How to import data into Google Data Studio using Python," Google Data Studio has native support for a range of platforms, but there’s no reliable means of pushing data in from Python without going via another data source. Google BigQuery is one option, but the easiest for small datasets is to use Google Sheets, which is supported natively and is easy to use. Here’s how you can create a simple data pipeline to take data from MySQL using Python and Pandas and then push it into Google Sheets so you can access it in Google Data Studio. Install GSpread To get our data from Python to Google Data Studio we’re going to the GSpread Python package and use some Google  Sheets spreadsheets to store our data. You can install gspread easily via PyPi and can both write to and read from  Google Sheets using Python. However, you will need to go to the GSpread documentation and follow the instructions on how to create a Google  Service Account and then download the JSON service_account.json keyfile to ~/.config/gspread/service_account. json, so it can be used to authenticate your user. pip3 install gspread  Create and share a spreadsheet Now that GSpread is installed and you have your Google Service Account key in the right place, you can import the Gspread package, and create a service account connection. We’ll then use the create() function to create a new spreadsheet called “Data: Monthly sales by channel” and we’ll share it with a user and email them to let them know. As spreadsheets in GSpread are created by the service account user, it’s vital that you share them, otherwise they won’t appear in your Google Drive! import gspread gc = gspread.service_account()  sh = gc.create('Data: Monthly sales by channel') sh.share('email@example.com', perm_type='user', role='writer', email_message='Data: Monthly sales by channel')  Open the spreadsheet There are three ways to open a spreadsheet: via the name, via the key, or via the URL. I’d recommend that you use the open_by_key example, since the key is unique, unlike the name, and it can’t be broken if the sheet is renamed. Go to your Google Drive, find the spreadsheet and extract the key hash from the URL. It’s the long string of characters in the middle. sh = gc.open_by_key('kasd789yas98dyu89asud9au8sd98uas9d8uj89ass')  Create some named worksheets Just like regular Google Sheets, you can add named worksheets to the spreadsheets you create. We’ll create a couple of named worksheets in which to store our data and then we’ll delete the default Sheet1. We need to define the number of rows and columns and provide a number when we create the new worksheets. wks_all = sh.add_worksheet(title=""All"", rows=1000, cols=6) wks_amazon = sh.add_worksheet(title=""Amazon"", rows=1000, cols=6) sh.del_worksheet(sh.sheet1)  Import data from Python to Google Sheets Now we’ll use SQLAlchemy to write some SQL queries to query MySQL, fetch the data in Pandas and then push that data into each of the sheets we created in our Google Sheets spreadsheet. Any % symbols need to be escaped with an additional % in SQLAlchemy. import gspread import pandas as pd from sqlalchemy import create_engine  sh = gc.open_by_key('kasd789yas98dyu89asud9au8sd98uas9d8uj89ass')  engine = create_engine('mysql+pymysql://root:SecretPassword@172.17.0.2:3306/database_name')  query = """""" SELECT 	DATE_FORMAT(orders.date_created, '%%Y%%m') AS period, 	COUNT(DISTINCT(orders.id)) AS total_orders, 	COUNT(DISTINCT(orders.customer_id)) AS total_customers, 	SUM(orders.revenue) AS total_revenue, 	ROUND((SUM(orders.revenue) / COUNT(DISTINCT(orders.customer_id))),2) AS aov, 	channels.title AS channel FROM shoporders LEFT JOIN channels ON orders.channel_id = channels.id WHERE channels.title = 'Amazon' GROUP BY DATE_FORMAT(orders.date_created, '%%Y%%m') ORDER BY DATE_FORMAT(orders.date_created, '%%Y%%m') DESC """"""  df = pd.read_sql(query, con=engine)  wks_all.update([df.columns.values.tolist()] + df.values.tolist())  You should now find your populated spreadsheet in your Google Drive. If you now go to Google Data Studio, you can add the data source in the usual way, giving you access to your MySQL, Pandas, or Python data directly in GDS. Matt Clarke, Thursday, March 04, 2021 "
1,https://practicaldatascience.co.uk/data-science/16-python-web-scraping-projects-for-ecommerce-and-seo,16 Python web scraping projects for ecommerce and SEO," Web scraping is a programming technique that uses a script or bot to visit one or more websites and extract specific elements or HTML tags from the source code of the page, so the data can be analysed, visualised, or used in models. Web scrapers have a multitude of uses, especially in SEO, and learning how to build them can give you access to data that can help improve your ecommerce or marketing team’s results. If you’re using Python for SEO, then you’ll be pleased to know that there are a range of excellent open source tools that make web scraping projects very simple for technically-minded SEOs, or data scientists supporting internal SEO teams. Python is the most widely using programming language for web scraping projects and the Python community has created some incredible applications that are well-suited to those working in SEO, marketing, or ecommerce, and have some reasonable Python programming skills to apply them to their work. How web scraping works The web scraping process involves two main steps: web crawling and web scraping. The web crawling step is the action of visiting a website and visiting every URL found, either by using a pre-generated list of URLs to crawl (such as those obtained when you scrape a sitemap.xml file), or by being given the domain as a starting point and then visiting every URL found via a process also known as web spidering. On each URL found by the web crawler (or web spider), some custom code then runs to “scrape” the desired content from the page’s underlying source code (usually HTML), using rules that identify specific HTML tags in the page, such as the title or meta description. The parsed data are then saved in a CSV file or database. The web scraping process therefore includes two different elements - one to make an HTTP request to the server to fetch the page, and one to parse the page’s source code to extract the elements of interest, usually using code that utilises regular expressions or Document Object Model (DOM) technologies such as XPath or CSS identifiers. While these can both be relatively complex to do by hand, Python does make the web scraping process much easier and there are now a wide range of excellent Python packages available to help you scrape websites and extract useful information to use in your analyses, visualisations, or models. Python web scraping packages Web scraping packages can be loosely divided into those that crawl websites, those that scrape or parse the content from the crawled pages, and those that combine the two. Some web scraping packages available are quite basic and easy to use for small projects, but are slower at scale, so you may need a more complex solution depending on the size of the sites you want to scrape. There’s usually no need to pay to access a costly web scraping API for most projects. You can usually build a custom web scraper for free, if you have some intermediate Python skills.   Requests Requests is a Python HTTP client package that can be used for making HTTP requests. The Requests library is very simple to use and can allow you to access the source code of a HTML document so it can be parsed using a web parsing package, such as Beautiful Soup. Requests is really good, but is best suited to smaller projects, as it only handles a single connection at a time, making it slower on larger sites.   Beautiful Soup Beautiful Soup is a parser package that can be used to read HTML, XML, RSS, and various other markup languages to allow you to extract specific page elements using either XPath or CSS selectors. The vast majority of web scraping packages make use of this Python library to provide their web parsing functionality, but you can also use it on its own to extract data from within chunks of HTML code. There's no specific requirement to access a whole HTML document when using Beautiful Soup.    Request-HTML Requests-HTML is a Python library that essentially combines the Requests library and Beautiful Soup in a single package, allowing it to both an HTML document and its source code, then parse the source to extract data of interest. It has some really neat features and is a great choice for smaller Python web scraping projects.   Selenium WebDriver Selenium WebDriver is a headless web browser that allows you to scrape websites just as a web browser does. This sophisticated web scraper can do things most other web scraping packages can't, including interact with pages, login, click buttons, and various other things, including handling JavaScript. It includes a built-in parser, which works a little like Beautiful Soup, making it an all-in-one solution for web scraping. It's a popular way to circumvent any anti-scraping measures a site owner may have put in place. While it's an amazing tool, it can be slower than some other alternatives.    Screaming Frog Screaming Frog SEO Spider is a commercial application and runs as a desktop application on Windows, Mac, and Linux machines, and as a command line program. This web scraper is preconfigured to scrape many common page elements and has a stack of features to scrape other content, and integrate with other platforms such as Google Analytics and Google Search Console, so it's immensely powerful. It not an expensive product, so if you're regularly doing web scraping projects it's worth considering, especially if you want less technical staff to assist or handle scraping tasks on their own.   Scrapy Scrapy is probably the most sophisticated package for web scraping with Python. Unlike most others, it supports threading, so can create multiple connections to a web site and scrape several pages at once, making it by far the quickest. It can crawl and scrape pages at a tremendous rate. However, the downside is that it's much more time consuming to set up and typically requires a lot more code than other scrapers. The learning curve is also the steepest of those here, as you usually need to build a custom web scraper for each site.   Advertools Finally, there's Advertools. Advertools is a Python package built on top of Scrapy and is aimed at SEOs who want to perform web scraping projects. It gives you all the benefits of Scrapy but with a simpler structure and without the need to write so much code (plus a shed-load of other handy features unrelated to web scraping). Unlike Scrapy, it fetches a bunch of common page elements by default, to save you the hassle of writing the code to do this yourself. As it supports threading, it's much quicker than the un-threaded scrapers like Requests and Selenium. It's a really good choice.    Python web scraping projects 1. Build an ecommerce price scraper If you work in ecommerce, one of the most common web scraping projects you will want to undertake is to build a price scraper. Price scrapers crawl a selected list of your competitors’ websites and extract prices, SKUs, product names, and other useful information to help retailers compare their product prices and check that their goods are competitively priced against those of their rivals. While scraping product prices from an individual ecommerce website isn’t particularly difficult, it becomes laborious when you need to scrape numerous websites. It usually requires you to develop site-specific scrapers that extract product prices based on each site’s bespoke HTML markup. Since most scrapers break when the underlying source code of the scraped pages is changed, this becomes a time-consuming and expensive process and introduces lots of technical debt. To work around this problem, my preferred approach is to instead scrape product prices from metadata or microdata embedded within the page whenever possible. This structured data is added to most ecommerce product pages to help search engines extract product data to enrich search engine results pages or allow searchers to compare products via Google Shopping and other price comparison platforms. By developing a price scraper that extracts microdata or JSON-LD using schema.org markup, a single scraper can extract prices from many sites, avoiding the need to build a bespoke one for every competitor. The other major complexity with ecommerce price scraping is product matching. Simply scraping the product and price information is easy enough, but the trickiest bit is working out which prices are a like-for-like match for the products you sell. To do this, you’ll need to first build a product matching dataset and then create a machine learning product matching model. Related posts  How to create a product and price metadata scraper  2. Scrape your competitors’ product and service reviews Another insightful web scraping project is to scrape competitor reviews. This can help you benchmark your business performance against theirs, see how or if they respond to negative reviews, understand what customers like and dislike about the service of your rivals, and see what products they’re selling and in what volumes. You can even use it to estimate their sales. While you could just scrape product reviews directly from their websites, or extract them from the JSON-LD or microdata stored within each product page, the easiest way to access these reviews in bulk is to obtain them from reviews platforms such as Trustpilot and Feefo. The Feefo API also lets you download reviews directly to analyse products or service. Related posts  How to scrape JSON-LD competitor reviews using Extruct How to use the Feefo API for ecommerce competitor analysis  3. Scrape the technologies used on competitors’ websites You may also want to examine the technologies that your competitors are using on their sites, which you can do online using tools such as Builtwith. These examine the source code of the page and look for references to the underlying technologies, such as the ecommerce or blogging platform used, or the JavaScript plugins used. While scraping competitor technology data can be interesting, the data aren’t always that reliable, and the Python Builtwith package seems a bit hit-and-miss. However, your mileage may vary. Related posts  How to scrape competitor technology data using Python  4. Crawl a site for 404 errors and redirect chains Another useful thing you can do with Python web scraping packages is use them to crawl your websites to look for things that cause problems for SEO, such as 404 or “page not found” errors and 301 redirect chains. 404 errors, caused by the inclusion of broken links or images, harm the user experience and can send a signal to search engines that the site is poorly maintained. Redirect chains impact your “crawl budget”, which can mean that visiting search engine spiders examine fewer pages than they otherwise would, potentially impacting how many new pages are found, and how many updated pages get refreshed in the SERPs. By scraping the URLs from your site and then using the requests package to retrieve the HTTP status code for each page, you can quickly do a bulk scan of your site and identify pages with a 404 or have a 301 redirect chain so you can fix the problem. Related posts  How to scan a site for 404 errors and 301 redirect chains  5. Scrape and parse robots.txt files The robots.txt file sits at the document root of every website and is used to tell compliant crawlers what they can crawl and what they can’t. It can also be used to tell crawlers where the sitemap.xml file is located, and throttle or ban aggressive bots that may bring a site to its knees by crawling pages too quickly. You can use Python to scrape and parse robots.txt files and put that data into a Pandas dataframe so you can analyse it separately, removing the need to visit the site, view the robots.txt file and transfer the content to a file yourself. Related posts  How to scrape and parse a robots.txt file with Python  6. Scrape schema.org microdata with Python Schema.org was founded by the world’s largest search engine providers - Google, Microsoft, Yahoo, and Yandex - to help improve the user experience on search engines by encouraging website owners to create “structured data” that was much easier for them to crawl and parse. The microdata comes in various forms, but is usually microdata (embedded in the page’s HTML), JSON-LD, or more rarely, RDF-A. There are now a huge range of schema.org schemas, covering everything from products, reviews and promotions, to people, organizations, and recipes. While it’s often overlooked, it can save you a huge amount of time and effort to scrape and parse microdata instead of scraping page content directly. Schema.org microdata should adhere to the same format, so you can create a single scraper that can work across multiple sites, which massively reduces development and maintenance overheads. The first step is to identify schema.org metadata usage, so you can see which dialect and schemas are in use on the sites you want to scrape. Then you can use Extruct to scrape schema.org metadata from the page and store it in a Pandas dataframe, or write it to CSV or database. Related posts  How to use Extruct to identify schema.org metadata usage How to scrape schema.org metadata using Python  7. Scrape Open Graph meta data Open Graph was designed by Facebook to help web pages become rich objects with social graphs. Basically, it’s just another way for site owner’s to help improve the user experience on Facebook and other social media platforms by structuring the data to make it easier for Facebook to scrape and put into widgets and posts on users’ feeds. Since Open Graph data is embedded directly in the <head> of the HTML document, you can scrape it and store it just like any other data embedded in the code. Scraping Open Graph data can give you quick access to information such as the page title, description, image, or videos present. Related posts  How to scrape Open Graph protocol data using Python  8. Scrape and parse XML sitemaps XML sitemaps have been used on websites for well over a decade now. They are structured documents written in a recognised XML format and are designed to help search engines identify the pages present on a website so they can be crawled and indexed for search engine users to find. When undertaking web scraping projects in Python, scraping XML sitemaps is generally one of the most useful first steps, since it provides your crawler with an initial URL list to crawl and scrape. You can also use data scraped from XML sitemaps to analyse the site’s information architecture or IA and understand more about what content or products are present, and where the site owner is focusing its efforts. By parsing URL structures in Python you can build up a map of the site and its overall structure. My EcommerceTools package makes scraping the sitemap.xml file a one-line task. Related posts  How to parse XML sitemaps using Python How to parse URL structures using Python How to use EcommerceTools for technical SEO  9. Scrape page titles and descriptions Scraping page titles and descriptions is one of the most useful SEO tasks you can perform in Python. You can perform simple checks, such as ensuring the lengths of the title or description are neither too long or too short, or you can combine the data with other sources and identify a range of other things you can change to improve SEO. One practical and simple project I’ve been doing for years is to identify the keywords each page is ranking for via the Google Search Console API, selecting the top keyword phrase, and then checking whether the words are present in the page title or meta description - effectively allowing you to identify keyword opportunities for which you already rank. Many SEO tools will perform this check for you. In Ahrefs, this feature is called “Page and SERP titles do not match”, which is found under the Site Audit > All issues section. However, it’s easy to do in Python (or even PHP). By identifying the keywords you already rank for, but which are missing from either your page title or meta description, you can add the phrases and get quick and easy improvements in both rankings and click-through rate, because Google will put the phrases in bold, helping them to stand out. Related posts  How to scrape page titles and meta descriptions How to access the Google Search Console API using Python How to identify SEO keyword opportunities with Python  10. Scrape Google search results For decades, most SEO tools have scraped the Google search engine result pages (or SERPs) to help SEOs understand how their content is ranking for given keywords. While you can get similar information by querying the Google Search Console API with Python, you can get additional information by scraping the SERPs themselves. While tools that scrape the SERPs are ubiquitous, Google doesn’t like you doing it, so you’ll find that it’s a fiddly process, and you’ll only be able to scrape a small volume of pages before you are temporarily blocked. You can work around these temporary blocks by using proxies, but your underlying code may also require regular updates, since Google often changes the HTML of the results that can break hard-coded scrapers. If you want to learn the underlying web scraping techniques, I’d recommend trying to build your own Google SERP scraper with Python. However, for a really quick and easy solution, my EcommerceTools Python package lets you scrape Google search results in just three lines of code. Besides just scraping the title, description, and URL shown in the search results, you can also extract a whole load of other potentially useful information from Google’s SERPs. This includes featured snippets, People Also Ask questions, related searches, and the words that Google is highlighting in bold (which often reveal useful synonyms you should be using in your pages). Related posts  How to query the Google Search Console API with EcommerceTools How to scrape Google search results using Python How to scrape Google results in three lines of code How to scrape People Also Ask data using Python How to create a Google rank checker tool using Python  11. Create a keyword research tool using Google Autocomplete The Google Autocomplete suggestions are also a very useful thing to scrape and analyse. By scraping Google autocomplete suggestions for search terms you can create a simple keyword suggestion tool that shows you a bunch of related search terms. Related posts  How to identify SEO keywords using Google autocomplete  12. Extract questions from Google’s People Also Ask widget The People Also Ask widget on the Google search engine results is also a great source of potential keyword ideas for content writers. Including questions and answers in your content, or clearly defining things that users are searching for, can increase your chances of appearing in these value slots or just help you rank higher. Like the autocomplete suggestions, it’s dead easy to scrape using Python. Related posts  How to scrape People Also Ask data using Python  13. Find a site’s internal and external links By scraping a site’s internal and external links, you can analyse them to see which ones are orphans (with no links pointing to them), and which ones could be good candidates for linking from your other pages. One really useful technique is to use the scraped links to create a network graph showing how the pages are linked to each other. I use the excellent NetworkX package for this. It creates superb visualisations showing internal linking structures and, when combined with Bokeh, allows you to click on the nodes and edges to reveal further information. Related posts  How to identify internal and external links using Python  14. Download a site’s RSS feed with Python RSS feeds are used on many content-led websites, such as blogs, to provide a structured list of post titles, descriptions, authors, and other data that can be retrieved in RSS feed readers or read aloud by voice assistants. Therefore, just as with sitemap.xml and schema.org data, reading RSS feeds in Python can yield useful structured data for analysis or use in other projects. It’s particularly useful for constructing Natural Language Processing datasets. Related posts  How to read an RSS feed in Python  15. Create a search engine rank tracking tool Following on from the SERP scraper mentioned above, one similar application is to create a simple Google rank tracking tool with Python. These take a Python list of target keywords, fetch the Google search engine results, and return the top ranking page for the domain you want to track. They’re useful for basic monitoring, but you’ll likely find you quickly get blocked temporarily, as Google isn’t a fan of being scraped itself, which is ironic given that it obtains all its own data using the exact same techniques. That said, setup a cron job or Airflow data pipeline and you can collect and report on a small number of keywords quickly and easily. Related posts  How to create ecommerce data pipelines in Apache Airflow How to create a Google rank checker tool using Python  16. Create machine learning datasets using web scraping Finally, if you’re building machine learning models, web scraping is one of the most effective ways to create your own machine learning datasets. Whether you’re scraping a dataset based on text data, such as jobs, or scraping an image dataset to train a machine learning model, web scraping with Python will give you the tools you need to make this a fairly simple task. Related posts  How to create a UK data science jobs dataset How to create image datasets for machine learning models  Matt Clarke, Wednesday, November 03, 2021 "
2,https://practicaldatascience.co.uk/machine-learning/16-how-to-create-google-search-console-time-series-forecasts-using-neural-prophet,How to create Google Search Console time series forecasts using Neural Prophet," Time series forecasting uses machine learning to predict future values of time series data. In this project we’ll be using the Neural Prophet model to predict future values of Google Search Console search performance data. Neural Prophet is a time series forecasting model, powered by PyTorch, that uses a neural network to predict future values of time series data. It’s a great way to use machine learning to predict future values of time series data. It was inspired by the excellent Facebook Prophet model, but uses Gradient Descent for optimisation, allows autocorrelation via AR-Net, and lets you use lagged regressors via a separate Feed Forward Neural Network. It sounds pretty complicated, but it’s actually not that hard to use, and gives pretty good results. Install the packages To create our time series forecast of Google Search Console performance data we’ll require the Pandas library and two other Python packages. To fetch your Google Search Console data using the API we’ll be using my EcommerceTools Python package. This lets you query GSC using Python, and do a whole load of other useful things for ecommerce, marketing, and SEO projects. For the time series forecasting bit we’ll be using the Neural Prophet package. This was inspired by Facebook’s excellent Prophet time series forecasting model, but has a few extra features. It’s very easy to use, but it uses a PyTorch backend, so your Python environment will need to have this set up. I’m using the NVIDIA Data Science Stack Docker container, and Neural Prophet and PyTorch can run on it out of the box. !pip3 install neuralprophet[live] !pip3 install ecommercetools  Load the packages Once you’ve installed Neural Prophet and EcommerceTools, import the packages below. We’ll be using the seo module from EcommerceTools, the NeuralProphet module from Neural Prophet, and the set_random_seed feature to allow reproducible results between runs of the model. import pandas as pd from ecommercetools import seo from neuralprophet import NeuralProphet from neuralprophet import set_random_seed  set_random_seed(0)  Configure your Google Search Console API connection Next, you’ll need to configure some variables to pass to EcommerceTools, so it can authenticate against your Google Search Console API account. This is done using a Google Cloud Service Account via a client secrets JSON keyfile. Create a client secrets JSON keyfile, define the URL you want to query, and set the start date and end date. This site is still quite new, so I don’t have very much data to play with, but I would advise that you set the longest duration you can to obtain the best forecast. key = ""pds-client-secrets.json"" site_url = ""sc-domain:practicaldatascience.co.uk"" start_date = ""2021-03-01"" end_date = ""2021-10-31""   Fetch your Google Search Console data Now we need to create a “payload” dictionary that EcommerceTools can pass to the Google Search Console API. This needs to include the startDate, endDate, and a list of dimensions that must include the date, since we want our data grouped by day for the model. payload = {     'startDate': start_date,     'endDate': end_date,     'dimensions': [""date""], }  Once you’ve created your payload dictionary, you can pass it to the seo.query_google_search_console() function along with the key variable holding the path to your client secrets JSON keyfile, and the site_url variable holding the URL of the domain you want to query. The function returns a Pandas dataframe. If you print the head() of the dataframe you’ll see that it contains clicks, impressions, ctr, and position. You can forecast any of these metrics using the model, simply by modifying the code below. df = seo.query_google_search_console(key, site_url, payload) df.sort_values(by='date', ascending=False).head()        date clicks impressions ctr position     244 2021-10-31 458 18762 2.44 29.70   243 2021-10-30 355 17206 2.06 30.91   242 2021-10-29 650 25269 2.57 25.83   241 2021-10-28 819 29741 2.75 23.89   240 2021-10-27 864 31293 2.76 23.77     Reformat the data for the Neural Prophet model Like the Facebook Prophet model, Neural Prophet also requires that you reformat the input dataframe you pass to the model when training the time series forecast model. This needs to have two columns: ds holding the date, and y containing the target variable you want the model to forecast. We can use the Pandas rename() function to take our original df dataframe and rename the columns accordingly. We’ll initially forecast clicks by renaming the clicks column y and then saving the output dataframe to data to avoid overwriting the original. data = df.rename(columns={'date': 'ds', 'clicks': 'y'})[['ds', 'y']] data.head()        ds y     0 2021-03-01 0   1 2021-03-02 0   2 2021-03-03 0   3 2021-03-04 0   4 2021-03-05 0     Create the Neural Prophet forecasting model Neural Prophet includes various options that you can pass to the model to generate more accurate time series forecasts. Since my site doesn’t really have weekly or monthly seasonality, but traffic does vary during the week, I’ve set daily_seasonality to True. I’ve then used fit() to build the fit the model on the data using daily data frequency to match what we exported from Google Search Console. This model will run pretty quickly on a GPU. model = NeuralProphet(daily_seasonality=True) metrics = model.fit(data, freq=""D"")  INFO - (NP.utils.set_auto_seasonalities) - Disabling yearly seasonality. Run NeuralProphet with yearly_seasonality=True to override this. INFO - (NP.config.set_auto_batch_epoch) - Auto-set batch_size to 16 INFO - (NP.config.set_auto_batch_epoch) - Auto-set epochs to 256    HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=220.0), HTML(value='')))   INFO - (NP.utils_torch.lr_range_test) - lr-range-test results: steep: 1.00E-01, min: 8.02E-01 INFO - (NP.utils_torch.lr_range_test) - learning rate range test selected lr: 4.01E-01 Epoch[256/256]: 100%|██████████| 256/256 [00:06<00:00, 41.93it/s, SmoothL1Loss=0.00432, MAE=56.4, MSE=4.99e+3, RegLoss=0]  Create a future dataframe Now the model is built, we need to create another dataframe containing the dates for the future period. You can do this using the make_future_dataframe() function. I’ve set this to a duration of 365 days, so we forecast one year into the future. You can view the output of the future dataframe by printing the head(). future = model.make_future_dataframe(data, periods=365, n_historic_predictions=True) future.head()        ds y t y_scaled     0 2021-03-01 0 0.000000 0.0   1 2021-03-02 0 0.004098 0.0   2 2021-03-03 0 0.008197 0.0   3 2021-03-04 0 0.012295 0.0   4 2021-03-05 0 0.016393 0.0     Create the time series forecast Now we’ll create a forecast. To do this we simply pass the future dataframe to the predict() function. We can then pass the forecast output to the plot() function to view the time series forecast itself. The blue line represents the model’s forecast for the clicks variable we passed to the model. The black dots represent the actual values in the previous period. As you can see from my data, there’s a decent upward trend in the clicks, as well as an odd dip during August, which seemed to coincide with a Google algorithm change, and then a subsequent reversal of the impact a month or so later. forecast = model.predict(future)  model.plot(forecast)    Examine the forecast To examine the actual values that have been forecast, you can print the output of the forecast dataframe. The dataframe gives you the date, the actual value recorded y, plus various other components from the time series, including the trend, seasonality, and forecast value in yhat1. forecast.head()        ds y yhat1 residual1 trend season_weekly season_daily     0 2021-03-01 0 34.964188 34.9642 -92.075294 52.409801 74.629684   1 2021-03-02 0 32.497509 32.4975 -91.105042 48.972870 74.629684   2 2021-03-03 0 34.239559 34.2396 -90.134789 49.744667 74.629684   3 2021-03-04 0 19.703564 19.7036 -89.164528 34.238403 74.629684   4 2021-03-05 0 -28.811146 -28.8111 -88.194275 -15.246559 74.629684     forecast.tail()        ds y yhat1 residual1 trend season_weekly season_daily     605 2022-10-27 None 2765.863525 NaN 2656.995361 34.238403 74.629684   606 2022-10-28 None 2721.852539 NaN 2662.469482 -15.246559 74.629684   607 2022-10-29 None 2640.049805 NaN 2667.943115 -102.522743 74.629684   608 2022-10-30 None 2680.449951 NaN 2673.416748 -67.596420 74.629684   609 2022-10-31 None 2805.930176 NaN 2678.890625 52.409801 74.629684     Plot the forecast components The time series decomposition technique can be used to extract the underlying components from within your time series. Neural Prophet makes time series decomposition very easy. You simply pass the forecast dataframe to the plot_components() function and it will give you a breakdown of each component. As you can see, the trend component is growing steadily, apart from the odd Google algorithm related blip back in August. The weekly seasonality element shows that the site is busiest on a Monday and quietest on a Saturday, but it looks like lots of data scientists spending their Sunday relaxing by reading the website. The daily seasonality component shows that the peak hours are mid morning, and during normal office hours, with the commute hours being the quietest. components = model.plot_components(forecast)   Matt Clarke, Friday, November 05, 2021 "
3,https://practicaldatascience.co.uk/data-science/how-to-create-descriptive-statistics-using-the-pandas-describe-function,How to create descriptive statistics using the Pandas describe function," The Pandas describe() function generates descriptive statistics on the contents of a Pandas dataframe to show the central tendency, shape, distribution, and dispersion of variables. Examining descriptive statistics is the first task in any quantitative data analysis, and they’re very quick and easy to generate using Pandas. The describe() function is commonly used during the Exploratory Data Analysis or EDA step immediately after data is loaded into the dataframe and allows the data scientist to quickly understand the data within. Here’s a quick tutorial explaining how to use the describe() function. Load the packages First, open a Jupyter notebook and import the packages you’ll need. We’ll only need Pandas, which you’ll probably already have installed. If you need to install it you can enter the following command in a terminal: pip3 install pandas. import pandas as pd  Import the data Next, we’ll import data into Pandas from a CSV file using the read_csv() function to load a remote file from my GitHub repository of datasets. We’ll assign the data to a variable called df and print the first few rows of the dataframe using the head() function. Alternatively, you can load your own data, or create a Pandas dataframe from scratch. df = pd.read_csv('https://raw.githubusercontent.com/flyandlure/datasets/master/google-analytics.csv') df.head()        User Type Source Medium Browser Device Category Date Pageviews     0 New Visitor (direct) (none) Amazon Silk mobile 2020-07-31 3   1 New Visitor (direct) (none) Amazon Silk mobile 2020-07-14 1   2 New Visitor (direct) (none) Amazon Silk tablet 2020-07-14 1   3 New Visitor (direct) (none) Amazon Silk tablet 2020-08-07 1   4 New Visitor (direct) (none) Amazon Silk tablet 2020-08-12 1     Examine the data types in the dataframe To understand what the dataframe contains, we can use df.info(). The info() function prints out the data types of each column in the dataframe and counts the number of missing values in each column. df.info()  <class 'pandas.core.frame.DataFrame'> RangeIndex: 10000 entries, 0 to 9999 Data columns (total 7 columns):  #   Column           Non-Null Count  Dtype  ---  ------           --------------  -----   0   User Type        10000 non-null  object  1   Source           10000 non-null  object  2   Medium           10000 non-null  object  3   Browser          10000 non-null  object  4   Device Category  10000 non-null  object  5   Date             10000 non-null  object  6   Pageviews        10000 non-null  int64  dtypes: int64(1), object(6) memory usage: 547.0+ KB  Use the describe() function to generate descriptive statistics Next, we’ll use the Pandas describe() function to generate descriptive statistics for each column in the dataframe. By default, the describe() function will return descriptive statistics on the numeric columns in your dataframe only. In the example dataframe above, only the Pageviews column contains numeric data. For each numeric column found in the dataframe, the describe() function will return the count, mean, standard deviation, minimum value, maximum value, and the 25%, 50% and 75% percentile values. df.describe()        Pageviews     count 10000.000000   mean 1.447600   std 0.972393   min 1.000000   25% 1.000000   50% 1.000000   75% 2.000000   max 14.000000     Generate descriptive statistics of the categorical columns What many data scientists don’t realise is that you can also use describe() on dataframes that include categorical or non-numeric variables, such as object data types. To use this method you pass in the additional include='all argument to ensure that all columns in the input dataframe are included in the output descriptive statistics. In addition to the values returned by the default describe() function with no arguments, df.describe(include='all') will also return some additional statistics for categorical data columns. These include unique showing the number of unique values in the column, top showing the most common value in the column, and freq showing the frequency of the top value within the column. df.describe(include='all')        User Type Source Medium Browser Device Category Date Pageviews     count 10000 10000 10000 10000 10000 10000 10000.000000   unique 1 19 4 17 3 30 NaN   top New Visitor google organic Chrome desktop 2020-08-03 NaN   freq 10000 6225 7509 6869 4882 395 NaN   mean NaN NaN NaN NaN NaN NaN 1.447600   std NaN NaN NaN NaN NaN NaN 0.972393   min NaN NaN NaN NaN NaN NaN 1.000000   25% NaN NaN NaN NaN NaN NaN 1.000000   50% NaN NaN NaN NaN NaN NaN 1.000000   75% NaN NaN NaN NaN NaN NaN 2.000000   max NaN NaN NaN NaN NaN NaN 14.000000     Transpose the dataframe to improve readability If you are examining the descriptive statistics on a very large dataframe, you may want to use the Pandas transpose option by appending .T after the function is called. This flips the orientation of the rows and columns and can make larger dataframes much easier to read. df.describe(include='all').T        count unique top freq mean std min 25% 50% 75% max     User Type 10000 1 New Visitor 10000 NaN NaN NaN NaN NaN NaN NaN   Source 10000 19 google 6225 NaN NaN NaN NaN NaN NaN NaN   Medium 10000 4 organic 7509 NaN NaN NaN NaN NaN NaN NaN   Browser 10000 17 Chrome 6869 NaN NaN NaN NaN NaN NaN NaN   Device Category 10000 3 desktop 4882 NaN NaN NaN NaN NaN NaN NaN   Date 10000 30 2020-08-03 395 NaN NaN NaN NaN NaN NaN NaN   Pageviews 10000.0 NaN NaN NaN 1.4476 0.972393 1.0 1.0 1.0 2.0 14.0     Generate descriptive statistics for specific columns If you only want to generate descriptive statistics for specific columns in your Pandas dataframe you can specify the list by placing this within double square brackets. For example, df[['Source', 'Medium']] will filter the Pandas dataframe to show only the two specified columns, and you can then append .describe() to view their summary statistics. df[['Source', 'Medium']].describe()        Source Medium     count 10000 10000   unique 19 4   top google organic   freq 6225 7509     Generate descriptive statistics for specific data types Another useful technique is to use the describe() function to show only data of a specific data type. To do this you can pass a list of data types to the include argument when calling describe(). If you run df.info() first it will tell you what data types are present within the dataframe. You can then specify the list and pass it like this: df.describe(include=['object']). df.info()  <class 'pandas.core.frame.DataFrame'> RangeIndex: 10000 entries, 0 to 9999 Data columns (total 7 columns):  #   Column           Non-Null Count  Dtype  ---  ------           --------------  -----   0   User Type        10000 non-null  object  1   Source           10000 non-null  object  2   Medium           10000 non-null  object  3   Browser          10000 non-null  object  4   Device Category  10000 non-null  object  5   Date             10000 non-null  object  6   Pageviews        10000 non-null  int64  dtypes: int64(1), object(6) memory usage: 547.0+ KB  df.describe(include=['object'])        User Type Source Medium Browser Device Category Date     count 10000 10000 10000 10000 10000 10000   unique 1 19 4 17 3 30   top New Visitor google organic Chrome desktop 2020-08-03   freq 10000 6225 7509 6869 4882 395     Changing the percentile values to use If we go back to our default describe(include='all') output we’ll see that it includes: count, unique, top, freq, mean, std, min, max and percentile values for 25%, 50%, and 75%. df.describe(include='all').T        count unique top freq mean std min 25% 50% 75% max     User Type 10000 1 New Visitor 10000 NaN NaN NaN NaN NaN NaN NaN   Source 10000 19 google 6225 NaN NaN NaN NaN NaN NaN NaN   Medium 10000 4 organic 7509 NaN NaN NaN NaN NaN NaN NaN   Browser 10000 17 Chrome 6869 NaN NaN NaN NaN NaN NaN NaN   Device Category 10000 3 desktop 4882 NaN NaN NaN NaN NaN NaN NaN   Date 10000 30 2020-08-03 395 NaN NaN NaN NaN NaN NaN NaN   Pageviews 10000.0 NaN NaN NaN 1.4476 0.972393 1.0 1.0 1.0 2.0 14.0     Add additional custom percentiles However, it’s also possible to define additional percentiles such as 10%, 20%, and 80% if you want to get an idea of the spread of data in different areas. To define specific additional percentiles you need to pass a list of values to the percentiles argument when calling the describe() function, using a decimal value to indicate the percentage, for example .10 for 10%. df.describe(include='all', percentiles=[.10, .20, .80]).T        count unique top freq mean std min 10% 20% 50% 80% max     User Type 10000 1 New Visitor 10000 NaN NaN NaN NaN NaN NaN NaN NaN   Source 10000 19 google 6225 NaN NaN NaN NaN NaN NaN NaN NaN   Medium 10000 4 organic 7509 NaN NaN NaN NaN NaN NaN NaN NaN   Browser 10000 17 Chrome 6869 NaN NaN NaN NaN NaN NaN NaN NaN   Device Category 10000 3 desktop 4882 NaN NaN NaN NaN NaN NaN NaN NaN   Date 10000 30 2020-08-03 395 NaN NaN NaN NaN NaN NaN NaN NaN   Pageviews 10000.0 NaN NaN NaN 1.4476 0.972393 1.0 1.0 1.0 1.0 2.0 14.0     Matt Clarke, Saturday, November 27, 2021 "
4,https://practicaldatascience.co.uk/data-science/how-to-use-the-pandas-apply-function,How to use the Pandas apply function on dataframe rows and columns," The Pandas apply() function allows you to run custom functions on the values in a Series or column of your Pandas dataframe. The Pandas apply function can be used for a wide range of data science tasks including Exploratory Data Analysis (or EDA) and in the feature engineering process that precedes machine learning model training. In this tutorial I’ll show you how you can use the Pandas apply function to run a function on a column of your dataframe, and run a function on several columns for each row in your dataframe. It’s a powerful tool for data cleaning and feature engineering and is very easy to learn. Here’s how it works. Load the package First, open a Jupyter notebook and import the pandas package. If you don’t have this installed you can install it with the following command: pip3 install pandas, or by entering !pip3 install pandas in a cell in the Jupyter notebook. We’ll also use the Pandas set_option() function to increase the maximum number of columns that Pandas will display when printing a dataframe. import pandas as pd pd.set_option('max_columns', 100)  Load the data Next, we’ll import data into Pandas by loading a remote CSV of data from my datasets GitHub repository using the read_csv() function. We can then use the head() function to display the first few rows of the dataframe. df = pd.read_csv('https://raw.githubusercontent.com/flyandlure/datasets/master/telco.csv') df.head()        customerID gender SeniorCitizen Partner Dependents tenure PhoneService MultipleLines InternetService OnlineSecurity OnlineBackup DeviceProtection TechSupport StreamingTV StreamingMovies Contract PaperlessBilling PaymentMethod MonthlyCharges TotalCharges Churn     0 7590-VHVEG Female 0 Yes No 1 No No phone service DSL No Yes No No No No Month-to-month Yes Electronic check 29.85 29.85 No   1 5575-GNVDE Male 0 No No 34 Yes No DSL Yes No Yes No No No One year No Mailed check 56.95 1889.5 No   2 3668-QPYBK Male 0 No No 2 Yes No DSL Yes Yes No No No No Month-to-month Yes Mailed check 53.85 108.15 Yes   3 7795-CFOCW Male 0 No No 45 No No phone service DSL Yes No Yes Yes No No One year No Bank transfer (automatic) 42.30 1840.75 No   4 9237-HQITU Female 0 No No 2 Yes No Fiber optic No No No No No No Month-to-month Yes Electronic check 70.70 151.65 Yes     To get an idea of what columns and data types are present in the Pandas dataframe we will use the Pandas info() function. The info() function returns the names of the columns, their data types and the number of null values. df.info()  <class 'pandas.core.frame.DataFrame'> RangeIndex: 7043 entries, 0 to 7042 Data columns (total 21 columns):  #   Column            Non-Null Count  Dtype   ---  ------            --------------  -----    0   customerID        7043 non-null   object   1   gender            7043 non-null   object   2   SeniorCitizen     7043 non-null   int64    3   Partner           7043 non-null   object   4   Dependents        7043 non-null   object   5   tenure            7043 non-null   int64    6   PhoneService      7043 non-null   object   7   MultipleLines     7043 non-null   object   8   InternetService   7043 non-null   object   9   OnlineSecurity    7043 non-null   object   10  OnlineBackup      7043 non-null   object   11  DeviceProtection  7043 non-null   object   12  TechSupport       7043 non-null   object   13  StreamingTV       7043 non-null   object   14  StreamingMovies   7043 non-null   object   15  Contract          7043 non-null   object   16  PaperlessBilling  7043 non-null   object   17  PaymentMethod     7043 non-null   object   18  MonthlyCharges    7043 non-null   float64  19  TotalCharges      7043 non-null   object   20  Churn             7043 non-null   object  dtypes: float64(1), int64(2), object(18) memory usage: 1.1+ MB  We can then use the Pandas describe() function to get a summary of the data and print some descriptive statistics to give us a feel of what the data looks like. df.describe()        SeniorCitizen tenure MonthlyCharges     count 7043.000000 7043.000000 7043.000000   mean 0.162147 32.371149 64.761692   std 0.368612 24.559481 30.090047   min 0.000000 0.000000 18.250000   25% 0.000000 9.000000 35.500000   50% 0.000000 29.000000 70.350000   75% 0.000000 55.000000 89.850000   max 1.000000 72.000000 118.750000     Apply a function to each value in a dataframe column The Pandas apply() function can be used in two main ways: it can be used to run a function on a specific column or Series in a Pandas dataframe, or it can be used to run a function on several columns in each row of the dataframe. To start off, we will use the apply() function to run a function on a specific column in the dataframe. The apply() function takes a function as an argument and applies that function to each value in a column. We’ll create a column called annual_charges() that contains the annual charges for each customer by multiplying the monthly charges by 12. def annual_charges(monthly_charges):     return 12 * monthly_charges  To run the function, we’ll create a new dataframe column called AnnualCharges using df['AnnualCharges'] and then run the function using df['AnnualCharges'].apply(annual_charges). This take the MonthlyCharges column value in each row, pass it to the annual_charges() function and put the result in the AnnualCharges column. df['AnnualCharges'] = df['MonthlyCharges'].apply(annual_charges)  df[['customerID', 'tenure', 'MonthlyCharges', 'AnnualCharges']].head()        customerID tenure MonthlyCharges AnnualCharges     0 7590-VHVEG 1 29.85 358.2   1 5575-GNVDE 34 56.95 683.4   2 3668-QPYBK 2 53.85 646.2   3 7795-CFOCW 45 42.30 507.6   4 9237-HQITU 2 70.70 848.4     Apply a function to each row in dataframe column To run a function on each row in a column, we can use the apply() function. This time, our function will take the entire row as an argument, and we will need to define the column names we want to use. We’ll create a function called average_monthly_charges() that will multiply the value in the MonthlyCharges column by the value in the tenure column for every row in the Pandas dataframe. def average_monthly_charges(row):     return row['MonthlyCharges'] * row['tenure']  To run the function we use a similar approach as we did when using apply() on a single column, but we additionally pass in the axis=1 argument to tell Pandas to run the function on each row. df['ChargesPerAnnum'] = df.apply(average_monthly_charges, axis=1)  If you run the code, you’ll see that Pandas has now looped over all the rows in the dataframe and run the function on the two columns we specified, assigning the new column to the dataframe. df[['customerID', 'tenure', 'MonthlyCharges', 'ChargesPerAnnum', 'AnnualCharges']].head()        customerID tenure MonthlyCharges ChargesPerAnnum AnnualCharges     0 7590-VHVEG 1 29.85 29.85 358.2   1 5575-GNVDE 34 56.95 1936.30 683.4   2 3668-QPYBK 2 53.85 107.70 646.2   3 7795-CFOCW 45 42.30 1903.50 507.6   4 9237-HQITU 2 70.70 141.40 848.4     Matt Clarke, Saturday, November 27, 2021 "
5,https://practicaldatascience.co.uk/data-engineering/how-to-backup-a-mysql-database-with-mysqldump-ssh-and-scp,"How to backup a MySQL database using mysqldump, SSH and SCP"," If you need to create a backup of a remote MySQL database, you can use the mysqldump command. The mysqldump application is known as a client utility and installed alongside MySQL itself. When executed, mysqldump will create a dump file of the database that includes all the data and the structure of the database in INSERT statements that you can run on your local machine to create a local copy of the remote database. In this tutorial, I’ll work through a simple example of using mysqldump to create a local copy of a remote database. I’ll be connecting to a remote server running Ubuntu Server via SSH, running the mysqldump command to backup the database to a file, and then downloading that file to my local machine using SCP. 1. Login to the remote server using SSH First, open a terminal and use SSH to login to the remote server hosting your MySQL database. You will need to enter your remote server username before the server IP address using the @ symbol. After running the command enter your password to login. ssh matt@123.456.789.123  2. Use mysqldump to create a backup of your MySQL database Next, on the remote server, run the mysqldump command to create a backup of your MySQL database. Modify the command below and replace your_mysql_username with your MySQL username, and replace your_database_name with the name of your database. Then run the command. It may take a few minutes depending on the size of your database. The mysqldump command will export the entire MySQL database and write the contents to a .sql file that you can download. This can then be used to create a new instance of your MySQL database on another server. mysqldump -u your_mysql_username --password='xxxxxxxxxxx' your_database_name > your_database_name.sql  For the next step you’ll need the full path to the backup file, so enter pwd to obtain your present working directory. For example, mine is /home/matt. pwd  3. Logout of the SSH connection Logout of the SSH connection to your remote machine and return to the shell on your local machine by entering logout. This will return you to the shell on your local machine. logout  4. Use SCP to download the SQL backup to your local machine Finally, on your local machine, we’ll use the SCP or secure copy application to download the MySQL backup file from the server via SSH. In the command below we’re starting scp and creating an SSH connection for the user matt on the IP address 123.456.789.123. Once logged in, SCP will look for the file at /home/matt/your_database_name.sql on the remote server and download it to /home/work/backups/your_database_name.sql on the local machine. It will give you a useful progress bar, so you can track how much of the database has been downloaded. scp matt@123.456.789.123:/home/matt/your_database_name.sql /home/work/backups/your_database_name.sql  Creating a local copy of your MySQL database is a good idea for several reasons. Firstly, it keeps your data backed up. Secondly, it allows you to create your own development environment where you can safely run complex MySQL queries against your database without the risk of slowing down your production database. To create a local development environment for data science projects running on MySQL databases my preferred approach is to run MySQL in a Docker container. This is a great way to create a development environment for data science projects that can be run on a local machine without the need to install a full-blown server. Matt Clarke, Saturday, December 04, 2021 "
6,https://practicaldatascience.co.uk/data-science/how-to-read-an-xml-feed-into-a-pandas-dataframe,How to read an XML feed into a Pandas dataframe," XML feeds are a data format that uses Extensible Markup Language to provide structured data that can be read by search engines and online advertising providers. For example, a Google Shopping feed uses XML to provide structured data on the products a retailer sells so Google can use them to serve ads and create price comparisons. Google Shopping feeds are critical to the performance of most ecommerce retail businesses. They need to be correctly formatted and contain all the relevant data Google requires to serve ads on Google Shopping and elsewhere. Google sets strict criteria on what Google Shopping feeds should include so ecommerce retailers need to check them regularly to ensure everything is present and correct. The disadvantage of using XML for Google Shopping feeds is that they are inherently much harder for humans to read. While most technical people quickly pick up how to read simple markup languages like XML, reading them at scale and checking the data they contain is still quite cumbersome. In this project I’ll show you how you can build an XML feed parser that reads an XML feed, such as your Google Shopping feed, and parses it into a neatly formatted Pandas dataframe that you can analyse or write to a CSV file. Not only is it perfect for Google Shopping, but it will also work on almost any other XML feed you throw at it. Here’s how it works. Install the packages First, fire up a new Jupyter notebook. For this project we’ll be using Pandas, a few core Python libraries, and the Beautiful Soup HTML and XML parser library. The standard Python packages will already be installed, and you’ll likely already have Pandas installed. However, you can install anything you don’t have by entering the below commands into a cell in a Jupyter notebook. !pip3 install pandas !pip3 install bs4 !pip3 install lxml  Requirement already satisfied: pandas in /conda/envs/data-science-stack-2.5.1/lib/python3.7/site-packages (1.1.4) Requirement already satisfied: python-dateutil>=2.7.3 in /conda/envs/data-science-stack-2.5.1/lib/python3.7/site-packages (from pandas) (2.8.1) Requirement already satisfied: pytz>=2017.2 in /conda/envs/data-science-stack-2.5.1/lib/python3.7/site-packages (from pandas) (2020.4) Requirement already satisfied: numpy>=1.15.4 in /conda/envs/data-science-stack-2.5.1/lib/python3.7/site-packages (from pandas) (1.19.4) Requirement already satisfied: six>=1.5 in /conda/envs/data-science-stack-2.5.1/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0) Requirement already satisfied: bs4 in /conda/envs/data-science-stack-2.5.1/lib/python3.7/site-packages (0.0.1) Requirement already satisfied: beautifulsoup4 in /conda/envs/data-science-stack-2.5.1/lib/python3.7/site-packages (from bs4) (4.9.3) Requirement already satisfied: soupsieve>1.2; python_version >= ""3.0"" in /conda/envs/data-science-stack-2.5.1/lib/python3.7/site-packages (from beautifulsoup4->bs4) (2.0.1) Requirement already satisfied: lxml in /conda/envs/data-science-stack-2.5.1/lib/python3.7/site-packages (4.6.4)  Import the packages Now you have the required packages installed, you’ll need to import them using the commands below. To make the Pandas dataframe a little easier to read, I’ve passed in an additional set_option() command to increase the maximum number of rows shown. import typing import pandas as pd import urllib.request from urllib.parse import urlparse from bs4 import BeautifulSoup  pd.set_option('max_rows', 10000)  Get the XML feed source Next, we need to write a little function to fetch the XML source of the remote XML feed we want to parse. The easiest way to do this is via the requests package. Depending on your server configuration, you may need to define a dictionary of headers that include a User-Agent value, otherwise the server might reject your request. Once we have the response back from requests, we can use Beautiful Soup to parse what we find. We’ll use the lxml-xml XML parser library for this. If you don’t have this installed, you’ll get an error that says “Error: Couldn’t find a tree builder with the features you requested: lxml-xml. Do you need to install a parser library?”. To install the lxml-xml parser library on an Ubuntu Linux data science workstation you can install it by entering pip3 install lxml, if you missed the step above. def get_feed(url):     """"""Scrapes an XML feed from the provided URL and returns XML source.          Args:         url (string): Fully qualified URL pointing to XML feed.      Returns:         source (string): XML source of scraped feed.     """"""          try:         response = urllib.request.urlopen(urllib.request.Request(url, headers={'User-Agent': 'Mozilla'}))         source = BeautifulSoup(response, 'lxml-xml', from_encoding=response.info().get_param('charset'))         return source     except Exception as e:         print('Error: ' + str(e))  Next, enter the URL of the remote XML feed you want to download and pass it to the get_feed() function and assign the result to a variable called xml. After a few seconds, depending on the size of the XML file you’re downloading, you’ll get back the raw XML of the feed. I’ve skipped out the URL here as the Google Shopping feed I’m parsing is commercially sensitive. FEED_URL = 'https://yourdomain.com/google_shopping.xml' xml = get_feed(FEED_URL)  Extract the XML elements in each item To parse the XML feed we could simply define each element we want to extract, for example the title, and write specific code to handle each element. However, this is a bit repetitive and will only work on feeds that have an identical schema. Instead, we’ll identify the elements inside the XML feed so we can dynamically extract them, whatever the feed contains. The get_elements() function will do this for us. We’ll pass in the xml variable holding the XML file source and define the name of each parent element. In a Google Shopping feed this is called item, so I’ve set that as the default value. Google Shopping feeds use an RSS dialect based on the schema http://base.google.com/ns/1.0 version 2.0. If you look at the XML of a Google Shopping feed you’ll notice that many of the elements have a g: prefix, such as g:id. This is because they correspond to the schema namespace. When we use the find_all() function to extract the element names it will return them all. The namespace prefix won’t be shown but that’s not required for parsing the elements, so there’s no need to worry about the difference. def get_elements(xml, item='item'):          try:         items = xml.find_all(item)         elements = [element.name for element in items[0].find_all()]         return elements     except Exception as e:         print('Error: ' + str(e))  elements = get_elements(xml) elements  ['id',  'title',  'description',  'google_product_category',  'product_type',  'link',  'image_link',  'additional_image_link',  'additional_image_link',  'condition',  'availability',  'price',  'sale_price',  'sale_price_effective_date',  'brand',  'gtin',  'mpn',  'identifier_exists',  'gender',  'age_group',  'size',  'item_group_id',  'color',  'is_bundle',  'material',  'pattern',  'shipping_weight',  'multipack',  'adult',  'adwords_grouping',  'adwords_labels',  'adwords_redirect',  'unit_pricing_measure',  'unit_pricing_base_measure',  'energy_efficiency_class',  'online_only']  Parse an XML feed into a Pandas dataframe Now, we’ll create a function called feed_to_df() and will pass it the URL of our XML feed and define the parent elements we want to extract, which are called item in the Google Shopping XML feed. We’ll then use get_feed() to fetch the raw XML and get_elements() to extract the list of XML element names. We’ll then use if isinstance(elements, typing.List) to check that the elements variable contains a list of XML elements. If it does, we’ll use it to create the columns in an empty Pandas dataframe using df = pd.DataFrame(columns=elements) into which we’ll store a row for each item we find in the feed. We’ll use the find_all() function from Beautiful Soup to parse the XML and return all the item elements. We’ll then use a for loop to loop over each one. For each item, we’ll then look for each element name from our list and we’ll parse the value and store it in the row dictionary and append each row to the df dictionary. def feed_to_df(url, item='item'):          xml = get_feed(url)     elements = get_elements(xml)          if isinstance(elements, typing.List):         df = pd.DataFrame(columns=elements)              items = xml.find_all(item)                  for item in items:             row = {}             for element in elements:                 if xml.find(element):                     if item.find(element):                         row[element] = item.findNext(element).text                     else:                         row[element] = ''                 else:                     row[element] = ''                          df = df.append(row, ignore_index=True)         return df  If you run the feed_to_df() function you’ll see that the code fetches and parses the XML feed, dynamically generates a Pandas dataframe, puts each item in a row and each element in a column and populates the values. If the feed changes, or if you want to parse an XML feed with a different schema, there’s no need to rewrite the code. df = feed_to_df(FEED_URL)  df.head(1).T        0     id ABC1234   title Dell Precision 7770   description Dell Precision 7770 workstation   google_product_category    product_type    link https://www...   image_link https://www...   additional_image_link https://www...   additional_image_link https://www...   condition new   availability out of stock   price 2999.99 GBP   sale_price 2499.99 GBP   sale_price_effective_date 2021-01-01T00:00+0100/2099-01-01T00:00+0100   brand Dell   gtin    mpn AHBSI87388   identifier_exists TRUE   gender    age_group    size    item_group_id AHBSI87388   color    is_bundle false   material    pattern    shipping_weight    multipack    adult    adwords_grouping    adwords_labels    adwords_redirect    unit_pricing_measure    unit_pricing_base_measure    energy_efficiency_class    online_only      df.to_csv('current_feed.csv', index=False)  Matt Clarke, Saturday, December 11, 2021 "
7,https://practicaldatascience.co.uk/data-science/how-to-use-the-dropbox-api-with-python,How to use the Dropbox API with Python," Dropbox is one of the most widely used file storage and file sharing platforms and is used by a wide variety of businesses. Dropbox has put the effort into building a powerful API that allows you to utilise Dropbox in your own applications, and it’s ideal for Python automations. Using the Dropbox Python SDK, you can upload, download, share, and delete files from your Python script. It’s fairly simple to use and there’s loads of documentation available and a vibrant developer community. In this project, I’ll cover the main things you might need to do from within a Python automation, including connecting to Dropbox, listing files, downloading files, uploading files, and sharing files. Install the Dropbox API First, open a terminal and install the Python SDK via the PyPi package manager by entering pip3 install dropbox. This will download the latest version and the required dependencies. pip3 install dropbox  Import the packages Next, import the packages below. As well as the dropbox package itself, we’re importing AuthError from exceptions so we can throw more descriptive exception messages. We’ll also be using the pathlib library and the Pandas package for manipulating data. import pathlib import pandas as pd import dropbox from dropbox.exceptions import AuthError  Set up your Dropbox application In order to use the Dropbox API you will first need to go to the Dropbox developers site and create an application. You’ll need to create a permanent access token that you can use to authenticate your application against your Dropbox account. DROPBOX_ACCESS_TOKEN = 'xxxxxxxxxxxxx'  Connect to the Dropbox API Rather than just running the API commands, we’ll also create some reusable functions to serve as a simple wrapper around the regular Dropbox API. The first function we’re creating is called dropbox_connect() and will connect to the Dropbox API using the DROPBOX_ACCESS_TOKEN we’ve set up. def dropbox_connect():     """"""Create a connection to Dropbox.""""""      try:         dbx = dropbox.Dropbox(DROPBOX_ACCESS_TOKEN)     except AuthError as e:         print('Error connecting to Dropbox with access token: ' + str(e))     return dbx  Get a list of files in a Dropbox folder Next, we’ll create a function to get a list of files in a Dropbox folder. This function will take a dbx connection and a folder path as arguments. The folder path is the path to the folder you want to list the files in relative to the App folder of your Dropbox account. To make the file list a bit easier to handle from within your Python automation, I’ve used Pandas to turn the list of files into a DataFrame. The files will be sorted in descending order by the server_modified timestamp, so the most recent ones appear at the top. def dropbox_list_files(path):     """"""Return a Pandas dataframe of files in a given Dropbox folder path in the Apps directory.     """"""      dbx = dropbox_connect()      try:         files = dbx.files_list_folder(path).entries         files_list = []         for file in files:             if isinstance(file, dropbox.files.FileMetadata):                 metadata = {                     'name': file.name,                     'path_display': file.path_display,                     'client_modified': file.client_modified,                     'server_modified': file.server_modified                 }                 files_list.append(metadata)          df = pd.DataFrame.from_records(files_list)         return df.sort_values(by='server_modified', ascending=False)      except Exception as e:         print('Error getting list of files from Dropbox: ' + str(e))  Download a file from Dropbox Downloading a file from Dropbox using Python is slightly more complex. We’ll connect to Dropbox using dropbox_connect(), then we’ll use with open() to open a local filepath and write the file to it when it’s downloaded using the dbx.files_download() function. def dropbox_download_file(dropbox_file_path, local_file_path):     """"""Download a file from Dropbox to the local machine.""""""      try:         dbx = dropbox_connect()          with open(local_file_path, 'wb') as f:             metadata, result = dbx.files_download(path=dropbox_file_path)             f.write(result.content)     except Exception as e:         print('Error downloading file from Dropbox: ' + str(e))  Upload a file to Dropbox To upload a file to Dropbox using Python we ‘ll use the dbx.files_upload() function. We’ll use pathlib.Path() to create the right path to our local file we want to upload and will store it in local_file_path. We’ll use  with open() again to open the file for reading, then we’ll use dbx.files_upload() to upload the file to the Dropbox folder. If the file exists, it will be overwritten. def dropbox_upload_file(local_path, local_file, dropbox_file_path):     """"""Upload a file from the local machine to a path in the Dropbox app directory.      Args:         local_path (str): The path to the local file.         local_file (str): The name of the local file.         dropbox_file_path (str): The path to the file in the Dropbox app directory.      Example:         dropbox_upload_file('.', 'test.csv', '/stuff/test.csv')      Returns:         meta: The Dropbox file metadata.     """"""      try:         dbx = dropbox_connect()          local_file_path = pathlib.Path(local_path) / local_file          with local_file_path.open(""rb"") as f:             meta = dbx.files_upload(f.read(), dropbox_file_path, mode=dropbox.files.WriteMode(""overwrite""))              return meta     except Exception as e:         print('Error uploading file to Dropbox: ' + str(e))  Download a shareable link to a file in Dropbox Finally, we’ll create a shareable link to a file already in Dropbox. This is a great way to share a large file with colleagues using a URL, rather than having to download the file and upload it to their machine. Changing the URL parameter from ?dl=0 to ?dl=1 will return a shareable link to the file that allows downloading. def dropbox_get_link(dropbox_file_path):     """"""Get a shared link for a Dropbox file path.      Args:         dropbox_file_path (str): The path to the file in the Dropbox app directory.      Returns:         link: The shared link.     """"""      try:         dbx = dropbox_connect()         shared_link_metadata = dbx.sharing_create_shared_link_with_settings(dropbox_file_path)         shared_link = shared_link_metadata.url         return shared_link.replace('?dl=0', '?dl=1')     except dropbox.exceptions.ApiError as exception:         if exception.error.is_shared_link_already_exists():             shared_link_metadata = dbx.sharing_get_shared_links(dropbox_file_path)             shared_link = shared_link_metadata.links[0].url             return shared_link.replace('?dl=0', '?dl=1')  Matt Clarke, Saturday, December 18, 2021 "
